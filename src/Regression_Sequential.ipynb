{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "print(housing.feature_names)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, test_size = 0.2, random_state = 42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size = 0.3, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(30, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(keras.layers.Dense(30, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "             optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "             metrics=['mae'])\n",
    "\n",
    "## Initially let's run for 20 epochs\n",
    "model_history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "\n",
    "plt.show()\n",
    "## So we can see that there's still scope to tune the model and achieve better loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now run for 60 more epochs\n",
    "model_history = model.fit(X_train, y_train, epochs=60, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "\n",
    "plt.show()\n",
    "## So we can see that there's still scope to tune the model and achieve better loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try 60 more epochs\n",
    "model_history = model.fit(X_train, y_train, epochs=60, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz\n",
    "ann_viz(model,title=\"model_neural_network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model,rankdir='LR',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's to predictions\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred)\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notice that our predictions are okay, we can do a lot better\n",
    "## Let's try few techniques that will unlock little more potential of ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "## Defining my optimizer - Its just a refined form of SGD\n",
    "## ref - https://keras.io/optimizers/\n",
    "adam = keras.optimizers.Adam(lr=0.01, decay=0.0005)\n",
    "\n",
    "## Sequential model lets you add neural net layers one after another by calling function\n",
    "model_mlp = keras.models.Sequential()\n",
    "\n",
    "## Adding one layer having only 30 neuron\n",
    "##mAdding one layer having only 30 neuron\n",
    "## Notice our data has 8 input columns which goes into as the \"input_shape\" parameter\n",
    "## Notice the use of l2 regularizer\n",
    "model_mlp.add(keras.layers.Dense(30, input_shape=(8,), activation='relu'))\n",
    "model_mlp.add(keras.layers.Dense(30, activation='relu'))\n",
    "model_mlp.add(keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callbacks\n",
    "earlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, verbose=1)\n",
    "\n",
    "## Compiling the model and defining the loss function\n",
    "model_mlp.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "## Training neural nets\n",
    "model_mlp_history = model_mlp.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n",
    "                          epochs=200, batch_size=15, callbacks=[reduce_lr, earlystopper])\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f'time taken for execution: {end_time-start_time} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model_mlp,rankdir='LR',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the model structure\n",
    "import pydot\n",
    "tf.keras.utils.plot_model(model_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz\n",
    "ann_viz(model_mlp,title=\"mlp_neural_network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = model_mlp.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_mlp_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = model_mlp.predict(X_new)\n",
    "print(y_pred)\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's try the same with different params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout\n",
    "\n",
    "import datetime\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "## Defining my optimizer - Its just a refined form of SGD\n",
    "## ref - https://keras.io/optimizers/\n",
    "adam = keras.optimizers.Adam(lr=0.001, decay=0.0005)\n",
    "\n",
    "## Just a way to define neural nets. There are two ways sequential and functional\n",
    "## Sequential model lets you add neural net layers one after another by calling function\n",
    "model_norm = keras.models.Sequential()\n",
    "\n",
    "## Adding one layer having only one neuron\n",
    "## Notice our data has 8 input columns which goes into as the \"input_shape\" parameter\n",
    "## Notice the use of l2 regularizer\n",
    "model_norm.add(Dense(100, input_shape=(8,), activation='relu'))\n",
    "model_norm.add(Dense(100, activation='relu'))\n",
    "model_norm.add(Dense(1))\n",
    "\n",
    "\n",
    "## Callbacks\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5, verbose=1)\n",
    "\n",
    "## Compiling the model and defining loss function\n",
    "model_norm.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "## Training neural nets\n",
    "model_norm_history = model_norm.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n",
    "                          epochs=200, batch_size=50, callbacks=[reduce_lr, earlystopper])\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f'time taken for execution: {end_time-start_time} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the model structure\n",
    "import pydot\n",
    "keras.utils.plot_model(model_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model_norm,rankdir='LR',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test = model_norm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_norm_history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = model_norm.predict(X_new)\n",
    "print(y_pred)\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "env_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
